name: Daily Job Scraper
on:
  schedule:
    # Run every day at 9:00 AM UTC (adjust timezone as needed)
    - cron: "0 7 * * *"
  # Allow manual trigger
  workflow_dispatch:
  #   inputs:
  #     platforms:
  #       description: "Platforms to scrape (comma-separated: linkedin,bayt,indeed)"
  #       required: false
  #       default: "linkedin,bayt,indeed"
  #     roles:
  #       description: "Custom roles to search (comma-separated)"
  #       required: false
  #       default: ""

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    environment: job-scrapper-env
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.9"

      - name: Setup Chrome and ChromeDriver
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install requests selenium python-dotenv

      - name: Create config file
        run: |
          cat > config.json << EOF
          {
            "airtable": {
              "api_key": "${{ secrets.AIRTABLE_API_KEY }}",
              "base_id": "${{ secrets.AIRTABLE_BASE_ID }}",
              "table_name": "Jobs"
            },
            "slack": {
              "webhook_url": "${{ secrets.SLACK_WEBHOOK_URL }}"
            },
            "scraping": {
              "headless": true,
              "delay_between_requests": 2,
              "max_pages_per_site": 5
            }
          }
          EOF

      - name: Run job scraper
        run: |
          python script.py
        env:
          DISPLAY: :99.0

      - name: Upload logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            *.log
            screenshot*.png
          retention-days: 7
